{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79b59a1a",
   "metadata": {},
   "source": [
    "# SABINA COMMENT: Please start adding headers and sections to your notebooks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "387c665b-f530-4f4a-bc05-4b5504a170c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Read with encoding: utf-8-sig  |  shape=(39145, 255)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39145 entries, 0 to 39144\n",
      "Columns: 255 entries, Date to Unnamed: 254\n",
      "dtypes: float64(234), object(21)\n",
      "memory usage: 76.2+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/00/c54qzf757tzdyqs5hr4n94vc0000gn/T/ipykernel_48198/3688720642.py:6: DtypeWarning: Columns (0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('/Users/reynoldtakurachoruma/GSAF5(Sheet1-GSAF).csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path = \"/Users/reynoldtakurachoruma/Downloads/GSAF5 1(Sheet1-GSAF).csv\"\n",
    "\n",
    "for enc in [\"utf-8-sig\", \"cp1252\", \"latin1\"]:\n",
    "    try:\n",
    "        df = pd.read_csv('/Users/reynoldtakurachoruma/GSAF5(Sheet1-GSAF).csv')\n",
    "        print(f\" Read with encoding: {enc}  |  shape={df.shape}\")\n",
    "        break\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"failed with  {enc}: {e}\")\n",
    "\n",
    "df.head()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27f0c0b",
   "metadata": {},
   "source": [
    "## SABINA COMMENT: [EXAMPLE SECTION] Review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93d058f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>Location</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 245</th>\n",
       "      <th>Unnamed: 246</th>\n",
       "      <th>Unnamed: 247</th>\n",
       "      <th>Unnamed: 248</th>\n",
       "      <th>Unnamed: 249</th>\n",
       "      <th>Unnamed: 250</th>\n",
       "      <th>Unnamed: 251</th>\n",
       "      <th>Unnamed: 252</th>\n",
       "      <th>Unnamed: 253</th>\n",
       "      <th>Unnamed: 254</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14th October</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Columbia</td>\n",
       "      <td>Bolivar, del Isolate</td>\n",
       "      <td>Catagena Province</td>\n",
       "      <td>Swimming with sharks</td>\n",
       "      <td>Male child</td>\n",
       "      <td>M</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11th October</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Queensland</td>\n",
       "      <td>Cook Esplanade Thursday Island</td>\n",
       "      <td>Fishing/swimming</td>\n",
       "      <td>Samuel Nai</td>\n",
       "      <td>M</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7th October</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Australia</td>\n",
       "      <td>South Australia</td>\n",
       "      <td>Kangaroo Island</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>Lee Berryman</td>\n",
       "      <td>M</td>\n",
       "      <td>50+</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29th September</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Off California</td>\n",
       "      <td>Catalina Island</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Christopher Murray</td>\n",
       "      <td>M</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27th September</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Provoked</td>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cocos Islands</td>\n",
       "      <td>Diving-Tagging sharks</td>\n",
       "      <td>Dr. Mauricio Hoyos</td>\n",
       "      <td>M</td>\n",
       "      <td>48</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 255 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date    Year        Type     Country                  State  \\\n",
       "0    14th October  2025.0  Unprovoked    Columbia   Bolivar, del Isolate   \n",
       "1    11th October  2025.0  Unprovoked   Australia             Queensland   \n",
       "2     7th October  2025.0  Unprovoked   Australia        South Australia   \n",
       "3  29th September  2025.0  Unprovoked         USA         Off California   \n",
       "4  27th September  2025.0    Provoked  Costa Rica                    NaN   \n",
       "\n",
       "                         Location               Activity                Name  \\\n",
       "0               Catagena Province   Swimming with sharks          Male child   \n",
       "1  Cook Esplanade Thursday Island       Fishing/swimming          Samuel Nai   \n",
       "2                 Kangaroo Island                Surfing        Lee Berryman   \n",
       "3                 Catalina Island               Swimming  Christopher Murray   \n",
       "4                   Cocos Islands  Diving-Tagging sharks  Dr. Mauricio Hoyos   \n",
       "\n",
       "  Sex  Age  ... Unnamed: 245 Unnamed: 246 Unnamed: 247 Unnamed: 248  \\\n",
       "0   M   14  ...          NaN          NaN          NaN          NaN   \n",
       "1   M   14  ...          NaN          NaN          NaN          NaN   \n",
       "2   M  50+  ...          NaN          NaN          NaN          NaN   \n",
       "3   M   54  ...          NaN          NaN          NaN          NaN   \n",
       "4   M   48  ...          NaN          NaN          NaN          NaN   \n",
       "\n",
       "  Unnamed: 249 Unnamed: 250 Unnamed: 251 Unnamed: 252 Unnamed: 253  \\\n",
       "0          NaN          NaN          NaN          NaN          NaN   \n",
       "1          NaN          NaN          NaN          NaN          NaN   \n",
       "2          NaN          NaN          NaN          NaN          NaN   \n",
       "3          NaN          NaN          NaN          NaN          NaN   \n",
       "4          NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "  Unnamed: 254  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "\n",
       "[5 rows x 255 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58d62645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'Year', 'Type', 'Country', 'State', 'Location', 'Activity',\n",
      "       'Name', 'Sex', 'Age',\n",
      "       ...\n",
      "       'Unnamed: 245', 'Unnamed: 246', 'Unnamed: 247', 'Unnamed: 248',\n",
      "       'Unnamed: 249', 'Unnamed: 250', 'Unnamed: 251', 'Unnamed: 252',\n",
      "       'Unnamed: 253', 'Unnamed: 254'],\n",
      "      dtype='object', length=255)\n"
     ]
    }
   ],
   "source": [
    "# print columns names \n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98de70c2",
   "metadata": {},
   "source": [
    "### SABINA COMMENT: [EXAMPLE SECTION] Remove empty columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6f3ed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all columns where all values are missing (NAN)\n",
    "# axis=1 specifies that we are operating on columns.\n",
    "df_cleaned = df.dropna(axis=1, how='all', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cf87a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new column count dropping empty columns :\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "print(\"new column count dropping empty columns :\")\n",
    "print(df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67a65399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>Location</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>...</th>\n",
       "      <th>Species</th>\n",
       "      <th>Source</th>\n",
       "      <th>pdf</th>\n",
       "      <th>href formula</th>\n",
       "      <th>href</th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Case Number.1</th>\n",
       "      <th>original order</th>\n",
       "      <th>Unnamed: 21</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14th October</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Columbia</td>\n",
       "      <td>Bolivar, del Isolate</td>\n",
       "      <td>Catagena Province</td>\n",
       "      <td>Swimming with sharks</td>\n",
       "      <td>Male child</td>\n",
       "      <td>M</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>Nurse shark</td>\n",
       "      <td>Kevin McMurray Trackingsharks.com Andy Currie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11th October</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Queensland</td>\n",
       "      <td>Cook Esplanade Thursday Island</td>\n",
       "      <td>Fishing/swimming</td>\n",
       "      <td>Samuel Nai</td>\n",
       "      <td>M</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>Tiger or Bull shark</td>\n",
       "      <td>Kevin McMurray Trackingsharks.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7th October</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Australia</td>\n",
       "      <td>South Australia</td>\n",
       "      <td>Kangaroo Island</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>Lee Berryman</td>\n",
       "      <td>M</td>\n",
       "      <td>50+</td>\n",
       "      <td>...</td>\n",
       "      <td>Bronze whaler?</td>\n",
       "      <td>Kevin McMurray Trackingsharks.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29th September</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Off California</td>\n",
       "      <td>Catalina Island</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Christopher Murray</td>\n",
       "      <td>M</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>unknown 1.2m shark</td>\n",
       "      <td>Todd Smith: Kevin McMurray Trackingsharks.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27th September</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Provoked</td>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cocos Islands</td>\n",
       "      <td>Diving-Tagging sharks</td>\n",
       "      <td>Dr. Mauricio Hoyos</td>\n",
       "      <td>M</td>\n",
       "      <td>48</td>\n",
       "      <td>...</td>\n",
       "      <td>Tiger shark 4m</td>\n",
       "      <td>Todd Smith: Kevin McMurray Trackingsharks.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date    Year        Type     Country                  State  \\\n",
       "0    14th October  2025.0  Unprovoked    Columbia   Bolivar, del Isolate   \n",
       "1    11th October  2025.0  Unprovoked   Australia             Queensland   \n",
       "2     7th October  2025.0  Unprovoked   Australia        South Australia   \n",
       "3  29th September  2025.0  Unprovoked         USA         Off California   \n",
       "4  27th September  2025.0    Provoked  Costa Rica                    NaN   \n",
       "\n",
       "                         Location               Activity                Name  \\\n",
       "0               Catagena Province   Swimming with sharks          Male child   \n",
       "1  Cook Esplanade Thursday Island       Fishing/swimming          Samuel Nai   \n",
       "2                 Kangaroo Island                Surfing        Lee Berryman   \n",
       "3                 Catalina Island               Swimming  Christopher Murray   \n",
       "4                   Cocos Islands  Diving-Tagging sharks  Dr. Mauricio Hoyos   \n",
       "\n",
       "  Sex  Age  ...             Species   \\\n",
       "0   M   14  ...         Nurse shark    \n",
       "1   M   14  ...  Tiger or Bull shark   \n",
       "2   M  50+  ...       Bronze whaler?   \n",
       "3   M   54  ...   unknown 1.2m shark   \n",
       "4   M   48  ...       Tiger shark 4m   \n",
       "\n",
       "                                          Source  pdf href formula href  \\\n",
       "0  Kevin McMurray Trackingsharks.com Andy Currie  NaN          NaN  NaN   \n",
       "1              Kevin McMurray Trackingsharks.com  NaN          NaN  NaN   \n",
       "2              Kevin McMurray Trackingsharks.com  NaN          NaN  NaN   \n",
       "3  Todd Smith: Kevin McMurray Trackingsharks.com  NaN          NaN  NaN   \n",
       "4  Todd Smith: Kevin McMurray Trackingsharks.com  NaN          NaN  NaN   \n",
       "\n",
       "  Case Number Case Number.1 original order Unnamed: 21 Unnamed: 22  \n",
       "0         NaN           NaN            NaN         NaN         NaN  \n",
       "1         NaN           NaN            NaN         NaN         NaN  \n",
       "2         NaN           NaN            NaN         NaN         NaN  \n",
       "3         NaN           NaN            NaN         NaN         NaN  \n",
       "4         NaN           NaN            NaN         NaN         NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows to see the cleaner column header \n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1642c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Column Headers:\n",
      "['Date', 'Year', 'Type', 'Country', 'State', 'Location', 'Activity', 'Name', 'Sex', 'Age', 'Injury', 'Fatal Y/N', 'Time', 'Species', 'Source', 'pdf', 'href formula', 'href', 'Case Number', 'Case Number.1', 'original order', 'Unnamed: 21', 'Unnamed: 22']\n"
     ]
    }
   ],
   "source": [
    "# 1. Strip leading/trailing whitespace from ALL column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# 2. Check the new, clean list of columns\n",
    "print(\"Cleaned Column Headers:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a13cdad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current column count: 23\n",
      "Current Column Headers:\n",
      "['Date', 'Year', 'Type', 'Country', 'State', 'Location', 'Activity', 'Name', 'Sex', 'Age', 'Injury', 'Fatal Y/N', 'Time', 'Species', 'Source', 'pdf', 'href formula', 'href', 'Case Number', 'Case Number.1', 'original order', 'Unnamed: 21', 'Unnamed: 22']\n"
     ]
    }
   ],
   "source": [
    "# Print the current number of columns and the column list\n",
    "print(f\"Current column count: {df.shape[1]}\")\n",
    "print(\"Current Column Headers:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a14d4578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/00/c54qzf757tzdyqs5hr4n94vc0000gn/T/ipykernel_48198/3351100676.py:3: DtypeWarning: Columns (0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('/Users/reynoldtakurachoruma/GSAF5(Sheet1-GSAF).csv')\n",
      "/var/folders/00/c54qzf757tzdyqs5hr4n94vc0000gn/T/ipykernel_48198/3351100676.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"Date\"] = pd.to_datetime(df[\"Date\"], dayfirst=True)\n"
     ]
    },
    {
     "ename": "OutOfBoundsDatetime",
     "evalue": "Out of bounds nanosecond timestamp: 14th October, at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32mconversion.pyx:326\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.conversion._TSObject.ensure_reso\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mnp_datetime.pyx:683\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.np_datetime.convert_reso\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOverflowError\u001b[0m: result would overflow",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOutOfBoundsDatetime\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/reynoldtakurachoruma/GSAF5(Sheet1-GSAF).csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m\"\u001b[39m], dayfirst\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)   \n\u001b[1;32m      5\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mextract(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m1,2})\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)  \n\u001b[1;32m      7\u001b[0m bins   \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m11\u001b[39m,\u001b[38;5;241m14\u001b[39m,\u001b[38;5;241m17\u001b[39m,\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m24\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/tools/datetimes.py:1067\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1065\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1067\u001b[0m         values \u001b[38;5;241m=\u001b[39m convert_listlike(arg\u001b[38;5;241m.\u001b[39m_values, \u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1068\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39m_constructor(values, index\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[38;5;241m.\u001b[39mMutableMapping)):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/tools/datetimes.py:435\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_strptime_with_fallback(arg, name, utc, \u001b[38;5;28mformat\u001b[39m, exact, errors)\n\u001b[0;32m--> 435\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64(\n\u001b[1;32m    436\u001b[0m     arg,\n\u001b[1;32m    437\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[1;32m    438\u001b[0m     yearfirst\u001b[38;5;241m=\u001b[39myearfirst,\n\u001b[1;32m    439\u001b[0m     utc\u001b[38;5;241m=\u001b[39mutc,\n\u001b[1;32m    440\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    441\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    442\u001b[0m )\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n\u001b[1;32m    447\u001b[0m     out_unit \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdatetime_data(result\u001b[38;5;241m.\u001b[39mdtype)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/arrays/datetimes.py:2398\u001b[0m, in \u001b[0;36mobjects_to_datetime64\u001b[0;34m(data, dayfirst, yearfirst, utc, errors, allow_object, out_unit)\u001b[0m\n\u001b[1;32m   2395\u001b[0m \u001b[38;5;66;03m# if str-dtype, convert\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(data, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mobject_)\n\u001b[0;32m-> 2398\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m tslib\u001b[38;5;241m.\u001b[39marray_to_datetime(\n\u001b[1;32m   2399\u001b[0m     data,\n\u001b[1;32m   2400\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m   2401\u001b[0m     utc\u001b[38;5;241m=\u001b[39mutc,\n\u001b[1;32m   2402\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[1;32m   2403\u001b[0m     yearfirst\u001b[38;5;241m=\u001b[39myearfirst,\n\u001b[1;32m   2404\u001b[0m     creso\u001b[38;5;241m=\u001b[39mabbrev_to_npy_unit(out_unit),\n\u001b[1;32m   2405\u001b[0m )\n\u001b[1;32m   2407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2408\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[1;32m   2409\u001b[0m     \u001b[38;5;66;03m#  is in UTC\u001b[39;00m\n\u001b[1;32m   2410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result, tz_parsed\n",
      "File \u001b[0;32mtslib.pyx:414\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtslib.pyx:596\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtslib.pyx:571\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mconversion.pyx:332\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.conversion._TSObject.ensure_reso\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOutOfBoundsDatetime\u001b[0m: Out of bounds nanosecond timestamp: 14th October, at position 0"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('/Users/reynoldtakurachoruma/GSAF5(Sheet1-GSAF).csv')\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], dayfirst=True)   \n",
    "df[\"hour\"] = df[\"Time\"].astype(str).str.extract(r\"(\\d{1,2})\").astype(float)  \n",
    "\n",
    "bins   = [-1,4,8,11,14,17,20,24]\n",
    "labels = [\"Night/Other\",\"05:08\",\"09:11\",\"12:14\",\"15:17\",\"18:20\",\"Night/Other\"]\n",
    "df[\"hour_bin\"] = pd.cut(df[\"hour\"], bins=bins, labels=labels)\n",
    "def to_hour(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    s = str(x).lower().strip()\n",
    "\n",
    "    if \"morning\" in s:  return 9\n",
    "    if \"afternoon\" in s:return 15\n",
    "    if \"evening\" in s or \"dusk\" in s: return 19\n",
    "    if \"night\" in s:    return 23\n",
    "    if \"noon\" in s:     return 12\n",
    "    if \"dawn\" in s or \"sunrise\" in s: return 6\n",
    "    return np.nan\n",
    "\n",
    "df[\"hour\"] = df[\"Time\"].apply(to_hour)\n",
    "\n",
    "def hour_bin(h):\n",
    "    \n",
    "    if pd.isna(h): return \"Unknown\"\n",
    "    h = int(h)\n",
    "    if 5 <= h < 9:   return \"05:08\"\n",
    "    if 9 <= h < 12:  return \"09:11\"\n",
    "    if 12 <= h < 15: return \"12:14\"\n",
    "    if 15 <= h < 18: return \"15:17\"\n",
    "    if 18 <= h < 21: return \"18:20\"\n",
    "    return \"Night/Other\"\n",
    "\n",
    "df[\"hour_bin\"] = df[\"hour\"].apply(hour_bin)\n",
    "\n",
    "df[\"year\"]  = df[\"date\"].dt.year\n",
    "df[\"month\"] = df[\"date\"].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9be320",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\n",
    "    'Sex ': 'Sex',\n",
    "    'Species ': 'Species'\n",
    "}, inplace=True)\n",
    "\n",
    "# Final check of the clean column names\n",
    "print(\"\\nFinal, Cleaned Column Headers:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547a295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .info() to display the number of non-null values for each column\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565e5610",
   "metadata": {},
   "source": [
    "SABINA COMMENT: Are you sure that the Fatal Y/N column should be removed? What if the places where there are null values simply mean that the person survived?\n",
    "\n",
    "Please re-read the description of the dataset here for the answer: https://www.sharkattackfile.net/incidentlog.htm\n",
    "\n",
    "Also, as a general rule... DO NOT remove data before looking at it! What if Species/Source/Time are relevant data? What if they are more recent data, which could help you analyze what happened in the past 1-2 years with more accuracy?\n",
    "\n",
    "I agree with removing PDF, HREF formula, HREF, Case Number directly as these don't add to the analysis, but the rest you should have a look at before removing! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581681d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SABINA COMMENT: Love the variable name!\n",
    "# Small tip to make it shorter: cols_to_drop_sparsity\n",
    "columns_to_drop_sparsity = [ 'pdf', 'href formula', 'href', \n",
    "    'Case Number', 'original order' \n",
    "]\n",
    "\n",
    "df.drop(columns=columns_to_drop_sparsity, axis=1, inplace=True)\n",
    "\n",
    "print(f\"Columns remaining: {df.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c833a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'Country' is missing\n",
    "df.dropna(subset=['Country'], inplace=True)\n",
    "print(f\"Rows remaining after dropping null Countries: {df.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1459e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year'] = pd.to_numeric(df['Year'], errors='coerce')\n",
    "df = df.dropna(subset=['Year'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0caa34c",
   "metadata": {},
   "source": [
    "## Unique Values for Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01963e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Countries:\n",
      "['Columbia' 'Australia' 'USA' 'Costa Rica' 'Bahamas' 'Puerto Rico'\n",
      " 'French Polynesia' 'Spain' 'Canary Islands' 'South Africa' 'Vanuatu'\n",
      " 'Jamaica' 'Israel' 'Mexico' 'Maldives' 'Philippines' 'Turks and Caicos'\n",
      " 'Mozambique' 'New Caledonia' 'Egypt' 'Thailand' 'New Zealand' 'Hawaii'\n",
      " 'Honduras' 'Indonesia' 'Morocco' 'Belize' 'Maldive Islands' 'Tobago'\n",
      " 'AUSTRALIA' 'INDIA' 'TRINIDAD' 'BAHAMAS' 'SOUTH AFRICA' 'MEXICO'\n",
      " 'NEW ZEALAND' 'EGYPT' 'BELIZE' 'PHILIPPINES' 'Coral Sea' 'SPAIN'\n",
      " 'PORTUGAL' 'SAMOA' 'COLOMBIA' 'ECUADOR' 'FRENCH POLYNESIA'\n",
      " 'NEW CALEDONIA' 'TURKS and CaICOS' 'CUBA' 'BRAZIL' 'SEYCHELLES'\n",
      " 'ARGENTINA' 'FIJI' 'MeXICO' 'ENGLAND' 'JAPAN' 'INDONESIA' 'JAMAICA'\n",
      " 'MALDIVES' 'THAILAND' 'COLUMBIA' 'COSTA RICA'\n",
      " 'British Overseas Territory' 'CANADA' 'JORDAN' 'ST KITTS / NEVIS'\n",
      " 'ST MARTIN' 'PAPUA NEW GUINEA' 'REUNION ISLAND' 'ISRAEL' 'CHINA'\n",
      " 'IRELAND' 'ITALY' 'MALAYSIA' 'LIBYA' nan 'MAURITIUS' 'SOLOMON ISLANDS'\n",
      " 'ST HELENA, British overseas territory' 'COMOROS' 'REUNION'\n",
      " 'UNITED KINGDOM' 'UNITED ARAB EMIRATES' 'CAPE VERDE' 'Fiji'\n",
      " 'DOMINICAN REPUBLIC' 'CAYMAN ISLANDS' 'ARUBA' 'MOZAMBIQUE' 'PUERTO RICO'\n",
      " 'ATLANTIC OCEAN' 'GREECE' 'ST. MARTIN' 'FRANCE' 'TRINIDAD & TOBAGO'\n",
      " 'KIRIBATI' 'DIEGO GARCIA' 'TAIWAN' 'PALESTINIAN TERRITORIES' 'GUAM'\n",
      " 'NIGERIA' 'TONGA' 'SCOTLAND' 'CROATIA' 'SAUDI ARABIA' 'CHILE' 'ANTIGUA'\n",
      " 'KENYA' 'RUSSIA' 'TURKS & CAICOS' 'UNITED ARAB EMIRATES (UAE)' 'AZORES'\n",
      " 'SOUTH KOREA' 'MALTA' 'VIETNAM' 'MADAGASCAR' 'PANAMA' 'SOMALIA' 'NEVIS'\n",
      " 'BRITISH VIRGIN ISLANDS' 'NORWAY' 'SENEGAL' 'YEMEN' 'GULF OF ADEN'\n",
      " 'Sierra Leone' 'ST. MAARTIN' 'GRAND CAYMAN' 'Seychelles' 'LIBERIA'\n",
      " 'VANUATU' 'MEXICO ' 'HONDURAS' 'VENEZUELA' 'SRI LANKA' ' TONGA' 'URUGUAY'\n",
      " 'MICRONESIA' 'CARIBBEAN SEA' 'OKINAWA' 'TANZANIA' 'MARSHALL ISLANDS'\n",
      " 'EGYPT / ISRAEL' 'NORTHERN ARABIAN SEA' 'HONG KONG' 'EL SALVADOR'\n",
      " 'ANGOLA' 'BERMUDA' 'MONTENEGRO' 'IRAN' 'TUNISIA' 'NAMIBIA'\n",
      " 'NORTH ATLANTIC OCEAN' 'SOUTH CHINA SEA' 'BANGLADESH' 'PALAU'\n",
      " 'WESTERN SAMOA' 'PACIFIC OCEAN ' 'BRITISH ISLES' 'GRENADA' 'IRAQ'\n",
      " 'TURKEY' 'SINGAPORE' 'NEW BRITAIN' 'SUDAN' 'JOHNSTON ISLAND'\n",
      " 'SOUTH PACIFIC OCEAN' 'NEW GUINEA' 'RED SEA' 'NORTH PACIFIC OCEAN'\n",
      " 'FEDERATED STATES OF MICRONESIA' 'MID ATLANTIC OCEAN' 'ADMIRALTY ISLANDS'\n",
      " 'BRITISH WEST INDIES' 'SOUTH ATLANTIC OCEAN' 'PERSIAN GULF'\n",
      " 'RED SEA / INDIAN OCEAN' 'PACIFIC OCEAN' 'NORTH SEA' 'NICARAGUA '\n",
      " 'MALDIVE ISLANDS' 'AMERICAN SAMOA' 'ANDAMAN / NICOBAR ISLANDAS' 'GABON'\n",
      " 'MAYOTTE' 'NORTH ATLANTIC OCEAN ' 'THE BALKANS' 'SUDAN?' 'MARTINIQUE'\n",
      " 'INDIAN OCEAN' 'GUATEMALA' 'NETHERLANDS ANTILLES'\n",
      " 'NORTHERN MARIANA ISLANDS' 'IRAN / IRAQ' 'JAVA' 'SIERRA LEONE'\n",
      " ' PHILIPPINES' 'NICARAGUA' 'CENTRAL PACIFIC' 'SOLOMON ISLANDS / VANUATU'\n",
      " 'SOUTHWEST PACIFIC OCEAN' 'BAY OF BENGAL' 'MID-PACIFC OCEAN' 'SLOVENIA'\n",
      " 'CURACAO' 'ICELAND' 'ITALY / CROATIA' 'BARBADOS' 'MONACO' 'GUYANA'\n",
      " 'HAITI' 'SAN DOMINGO' 'KUWAIT' 'YEMEN ' 'FALKLAND ISLANDS' 'CRETE'\n",
      " 'CYPRUS' 'EGYPT ' 'WEST INDIES' 'BURMA' 'LEBANON' 'PARAGUAY'\n",
      " 'BRITISH NEW GUINEA' 'CEYLON' 'OCEAN' 'GEORGIA' 'SYRIA' 'TUVALU'\n",
      " 'INDIAN OCEAN?' 'GUINEA' 'ANDAMAN ISLANDS' 'EQUATORIAL GUINEA / CAMEROON'\n",
      " 'COOK ISLANDS' 'TOBAGO' 'PERU' 'AFRICA' 'ALGERIA' 'Coast of AFRICA'\n",
      " 'TASMAN SEA' 'GHANA' 'GREENLAND' 'MEDITERRANEAN SEA' 'SWEDEN' 'ROATAN'\n",
      " 'Between PORTUGAL & INDIA' 'DJIBOUTI' 'BAHREIN' 'KOREA' 'RED SEA?'\n",
      " 'ASIA?' 'CEYLON (SRI LANKA)']\n"
     ]
    }
   ],
   "source": [
    "# Find the unique values in the category columns \n",
    "unique_countries = df['Country'].unique()\n",
    "print(\"Unique Countries:\")\n",
    "print(unique_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d25e124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/00/c54qzf757tzdyqs5hr4n94vc0000gn/T/ipykernel_48198/914924191.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  df.country = df['Country'].str.strip().str.lower()\n"
     ]
    }
   ],
   "source": [
    "df.country = df['Country'].str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c9b221b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Country Names:\n",
      "['columbia' 'australia' 'usa' 'costa rica' 'bahamas' 'puerto rico'\n",
      " 'french polynesia' 'spain' 'canary islands' 'south africa' 'vanuatu'\n",
      " 'jamaica' 'israel' 'mexico' 'maldives' 'philippines' 'turks and caicos'\n",
      " 'mozambique' 'new caledonia' 'egypt' 'thailand' 'new zealand' 'hawaii'\n",
      " 'honduras' 'indonesia' 'morocco' 'belize' 'maldive islands' 'tobago'\n",
      " 'india' 'trinidad' 'coral sea' 'portugal' 'samoa' 'colombia' 'ecuador'\n",
      " 'cuba' 'brazil' 'seychelles' 'argentina' 'fiji' 'england' 'japan'\n",
      " 'british overseas territory' 'canada' 'jordan' 'st kitts / nevis'\n",
      " 'st martin' 'papua new guinea' 'reunion island' 'china' 'ireland' 'italy'\n",
      " 'malaysia' 'libya' nan 'mauritius' 'solomon islands'\n",
      " 'st helena, british overseas territory' 'comoros' 'reunion'\n",
      " 'united kingdom' 'united arab emirates' 'cape verde' 'dominican republic'\n",
      " 'cayman islands' 'aruba' 'atlantic ocean' 'greece' 'st. martin' 'france'\n",
      " 'trinidad & tobago' 'kiribati' 'diego garcia' 'taiwan'\n",
      " 'palestinian territories' 'guam' 'nigeria' 'tonga' 'scotland' 'croatia'\n",
      " 'saudi arabia' 'chile' 'antigua' 'kenya' 'russia' 'turks & caicos'\n",
      " 'united arab emirates (uae)' 'azores' 'south korea' 'malta' 'vietnam'\n",
      " 'madagascar' 'panama' 'somalia' 'nevis' 'british virgin islands' 'norway'\n",
      " 'senegal' 'yemen' 'gulf of aden' 'sierra leone' 'st. maartin'\n",
      " 'grand cayman' 'liberia' 'venezuela' 'sri lanka' 'uruguay' 'micronesia'\n",
      " 'caribbean sea' 'okinawa' 'tanzania' 'marshall islands' 'egypt / israel'\n",
      " 'northern arabian sea' 'hong kong' 'el salvador' 'angola' 'bermuda'\n",
      " 'montenegro' 'iran' 'tunisia' 'namibia' 'north atlantic ocean'\n",
      " 'south china sea' 'bangladesh' 'palau' 'western samoa' 'pacific ocean'\n",
      " 'british isles' 'grenada' 'iraq' 'turkey' 'singapore' 'new britain'\n",
      " 'sudan' 'johnston island' 'south pacific ocean' 'new guinea' 'red sea'\n",
      " 'north pacific ocean' 'federated states of micronesia'\n",
      " 'mid atlantic ocean' 'admiralty islands' 'british west indies'\n",
      " 'south atlantic ocean' 'persian gulf' 'red sea / indian ocean'\n",
      " 'north sea' 'nicaragua' 'american samoa' 'andaman / nicobar islandas'\n",
      " 'gabon' 'mayotte' 'the balkans' 'sudan?' 'martinique' 'indian ocean'\n",
      " 'guatemala' 'netherlands antilles' 'northern mariana islands'\n",
      " 'iran / iraq' 'java' 'central pacific' 'solomon islands / vanuatu'\n",
      " 'southwest pacific ocean' 'bay of bengal' 'mid-pacifc ocean' 'slovenia'\n",
      " 'curacao' 'iceland' 'italy / croatia' 'barbados' 'monaco' 'guyana'\n",
      " 'haiti' 'san domingo' 'kuwait' 'falkland islands' 'crete' 'cyprus'\n",
      " 'west indies' 'burma' 'lebanon' 'paraguay' 'british new guinea' 'ceylon'\n",
      " 'ocean' 'georgia' 'syria' 'tuvalu' 'indian ocean?' 'guinea'\n",
      " 'andaman islands' 'equatorial guinea / cameroon' 'cook islands' 'peru'\n",
      " 'africa' 'algeria' 'coast of africa' 'tasman sea' 'ghana' 'greenland'\n",
      " 'mediterranean sea' 'sweden' 'roatan' 'between portugal & india'\n",
      " 'djibouti' 'bahrein' 'korea' 'red sea?' 'asia?' 'ceylon (sri lanka)']\n"
     ]
    }
   ],
   "source": [
    "df['Country'] = df['Country'].str.strip().str.lower()\n",
    "print(\"Cleaned Country Names:\")\n",
    "print(df['Country'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75a56154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of unique countries remaining after cleaning is: 212\n"
     ]
    }
   ],
   "source": [
    "# Assuming we've already applied all the cleaning to df['Country']\n",
    "\n",
    "# Calculate the total number of unique values (countries)\n",
    "total_unique_countries = df['Country'].nunique()\n",
    "\n",
    "# Print the result\n",
    "print(f\"The total number of unique countries remaining after cleaning is: {total_unique_countries}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68e599c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique Countries after Cleaning (First 200):\n",
      "['Columbia' 'Australia' 'Usa' 'Costa Rica' 'Bahamas' 'Puerto Rico'\n",
      " 'French Polynesia' 'Spain' 'Canary Islands' 'South Africa' 'Vanuatu'\n",
      " 'Jamaica' 'Israel' 'Mexico' 'Maldives' 'Philippines' 'Turks And Caicos'\n",
      " 'Mozambique' 'New Caledonia' 'Egypt' 'Thailand' 'New Zealand' 'Hawaii'\n",
      " 'Honduras' 'Indonesia' 'Morocco' 'Belize' 'Maldive Islands' 'Tobago'\n",
      " 'India' 'Trinidad' 'Coral Sea' 'Portugal' 'Samoa' 'Colombia' 'Ecuador'\n",
      " 'Cuba' 'Brazil' 'Seychelles' 'Argentina' 'Fiji' 'England' 'Japan'\n",
      " 'British Overseas Territory' 'Canada' 'Jordan' 'St Kitts / Nevis'\n",
      " 'St Martin' 'Papua New Guinea' 'Reunion Island' 'China' 'Ireland' 'Italy'\n",
      " 'Malaysia' 'Libya' nan 'Mauritius' 'Solomon Islands'\n",
      " 'St Helena, British Overseas Territory' 'Comoros' 'Reunion'\n",
      " 'United Kingdom' 'United Arab Emirates' 'Cape Verde' 'Dominican Republic'\n",
      " 'Cayman Islands' 'Aruba' 'Atlantic Ocean' 'Greece' 'St. Martin' 'France'\n",
      " 'Trinidad & Tobago' 'Kiribati' 'Diego Garcia' 'Taiwan'\n",
      " 'Palestinian Territories' 'Guam' 'Nigeria' 'Tonga' 'Scotland' 'Croatia'\n",
      " 'Saudi Arabia' 'Chile' 'Antigua' 'Kenya' 'Russia' 'Turks & Caicos'\n",
      " 'United Arab Emirates (Uae)' 'Azores' 'South Korea' 'Malta' 'Vietnam'\n",
      " 'Madagascar' 'Panama' 'Somalia' 'Nevis' 'British Virgin Islands' 'Norway'\n",
      " 'Senegal' 'Yemen']\n"
     ]
    }
   ],
   "source": [
    "# Ensure all countries are in title case for consistency\n",
    "df['Country'] = df['Country'].str.title()\n",
    "\n",
    "# Print the remaining unique values to check the cleaning\n",
    "print(\"\\nUnique Countries after Cleaning (First 200):\")\n",
    "print(df['Country'].unique()[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ebc719e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Countries:\n",
      "0                 Columbia\n",
      "1                Australia\n",
      "3                      Usa\n",
      "4               Costa Rica\n",
      "9                  Bahamas\n",
      "               ...        \n",
      "6963               Bahrein\n",
      "6968                 Korea\n",
      "6976              Red Sea?\n",
      "7039                 Asia?\n",
      "7049    Ceylon (Sri Lanka)\n",
      "Name: Country, Length: 213, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Identifying unique Countries \n",
    "drop_duplicates = df['Country'].duplicated()\n",
    "unique_countries = df['Country'][~drop_duplicates]\n",
    "print(\"Unique Countries:\")\n",
    "print(unique_countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9846099",
   "metadata": {},
   "source": [
    "### Unique Values for Age "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15753245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force the 'Age' column to be numeric.\n",
    "# Any non-numeric string (like '50+' or 'Teen') will be converted to NaN.\n",
    "# SABINA COMMENT: No! I would check how much data I lose BEFORE converting to NaN. \n",
    "# This is good as a quick & dirty method to get insights QUICKLY, I agree and I applaud you for that.\n",
    "# But for a proper analysis, you want to check that you're not losing a lot of your data this way \n",
    "df['Age'] = pd.to_numeric(df['Age'], errors='coerce')\n",
    "\n",
    "# Re-run the median calculation and imputation\n",
    "median_age = df['Age'].median()\n",
    "\n",
    "# Fill missing Age values (including the new NaNs created above) with the median\n",
    "df['Age'].fillna(median_age, inplace=True)\n",
    "\n",
    "print(f\"Age column successfully cleaned and filled. Median Age used: {median_age}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6be0df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the unique values in the category columns \n",
    "unique_Age = df['Age'].unique()\n",
    "print(\"Unique Age:\")\n",
    "print(unique_Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec29648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numeric, turning any remaining garbage into NaN\n",
    "df['Cleaned_Age'] = pd.to_numeric(df['Cleaned_Age'], errors='coerce')\n",
    "\n",
    "# Drop rows where 'Age' is now NaN (the unusable data)\n",
    "rows_before_drop = len(df)\n",
    "df.dropna(subset=['Cleaned_Age'], inplace=True)\n",
    "rows_after_drop = len(df)\n",
    "\n",
    "print(f\"Dropped {rows_before_drop - rows_after_drop} rows with unrecoverable Age data.\")\n",
    "\n",
    "# Final conversion to integer\n",
    "df['Cleaned_Age'] = df['Cleaned_Age'].astype(int) \n",
    "\n",
    "# Finalize the column names\n",
    "df.drop('Age', axis=1, inplace=True)\n",
    "df.rename(columns={'Cleaned_Age': 'Age'}, inplace=True)\n",
    "print(\"\\n'Age' column is now clean and numeric.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ea7661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Data Type and Non-Null Count\n",
    "print(\"--- Final Data Info ---\")\n",
    "df.info() \n",
    "# Look for:\n",
    "# 'Age' column to be a numeric type (e.g., 'int64' or 'float64').\n",
    "# 'Non-Null Count' for 'Age' to match the total entries (or be very close), \n",
    "#    indicating that all NaN values were dropped.\n",
    "\n",
    "# Check: Missing Values Count\n",
    "print(\"\\nMissing values in 'Age' column:\", df['Age'].isna().sum())\n",
    "# This count should be 0 or very close to 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7f66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique Values (The critical check)\n",
    "print(\"\\nFirst 20 unique values in the cleaned 'Age' column:\")\n",
    "# Using .head(20) because there are likely many unique numbers.\n",
    "print(df['Age'].unique()[:20]) \n",
    "# The output should ONLY contain numbers (integers or floats). \n",
    "# Textual entries like 'teen', 'mid-30s', 'X' should be gone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdb388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force 'Year' to be numeric, turning any remaining bad text/data into NaN\n",
    "# SABINA COMMENT: Same comment, no blind coercion!\n",
    "df['Year'] = pd.to_numeric(df['Year'], errors='coerce')\n",
    "\n",
    "# Drop the rows that have a missing 'Year' (after coercion)\n",
    "# These are likely records with unusable data anyway.\n",
    "# SABINA COMMENT: NOOOOOOO! Same thing! It's good there isn't that much data missing after\n",
    "# you do it, but ALWAYS CHECK FIRST!\n",
    "df.dropna(subset=['Year'], inplace=True)\n",
    "\n",
    "# Now, we can safely convert to integer\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "# Imputation (just in case they were missed earlier)\n",
    "# Impute Missing Activity (Categorical)\n",
    "df['Activity'].fillna('Unknown', inplace=True)\n",
    "print(\"Missing Activity values filled with 'Unknown'.\")\n",
    "print(\"\\nFinal Data Info:\")\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1701add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to convert the cleaned 'Year' column to integer\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "print(\"Year column successfully converted to integer.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b18a34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use value_counts() to count the occurrences of each Country\n",
    "# Use .head(5) to display only the top 5\n",
    "top_countries = df['Country'].value_counts().head(5)\n",
    "# SABINA COMMENT: Did you check that the Country column was clean?\n",
    "\n",
    "print(\"--- Top 5 Countries by Recorded Shark Attacks ---\")\n",
    "print(top_countries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87529b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# The 'top_countries' variable already holds the data from previous step.\n",
    "# It is a Pandas Series, which can be plotted directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce4e9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'top_countries' exists; compute it from df if it doesn't.\n",
    "if 'top_countries' not in globals():\n",
    "    try:\n",
    "        top_countries = df['Country'].value_counts().head(5)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"top_countries is not defined and could not be computed from df: \" + str(e))\n",
    "\n",
    "plt.figure(figsize=(10, 6))  # Set the size of the plot\n",
    "\n",
    "# Create the bar chart\n",
    "top_countries.plot(kind='bar', color='skyblue')\n",
    "\n",
    "# Add titles and labels for clarity\n",
    "plt.title('Top 5 Countries by Recorded Shark Attacks', fontsize=16)\n",
    "plt.xlabel('Country', fontsize=12)\n",
    "plt.ylabel('Number of Attacks', fontsize=12)\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add the count value on top of each bar (use a small dynamic offset)\n",
    "offset = max(top_countries.values) * 0.02 if len(top_countries) else 0\n",
    "for i, v in enumerate(top_countries.values):\n",
    "    plt.text(i, v + offset, str(int(v)), ha='center')\n",
    "\n",
    "# Display the chart\n",
    "plt.tight_layout()  # Adjusts plot for a tight layout\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9da703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the 'Activity' column by converting all text to lowercase\n",
    "# and filling any remaining nulls with 'unknown' (though we imputed earlier, this is a good safety step)\n",
    "df['Activity_Clean'] = df['Activity'].str.lower().fillna('unknown')\n",
    "\n",
    "# Count the occurrences of each unique (lowercased) activity\n",
    "# Use .head(5) to display only the top 5\n",
    "top_activities = df['Activity_Clean'].value_counts().head(5)\n",
    "\n",
    "print(\"--- Top 5 Activities by Recorded Shark Attacks ---\")\n",
    "print(top_activities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac43a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the bar chart\n",
    "top_activities.plot(kind='bar', color='coral')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Top 5 Activities Associated with Shark Attacks', fontsize=16)\n",
    "plt.xlabel('Activity', fontsize=12)\n",
    "plt.ylabel('Number of Attacks', fontsize=12)\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add the count value on top of each bar\n",
    "for index, value in enumerate(top_activities):\n",
    "    plt.text(index, value + 20, str(value), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a21097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf3ebb5",
   "metadata": {},
   "source": [
    "SABINA COMMENT: I think you need to check this in the beginning - your Fatal column is probably split in multiple columns and you already lost the original Fatal col. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b948415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of columns does NOT contain 'Fatal (Y/N)'. \n",
    "# We will use the common alternative column 'Unnamed: 21' which often holds the Fatal flag 'Y' or 'F'.\n",
    "\n",
    "FATAL_COLUMN = 'Unnamed: 21' # <--- CHANGED TO A COLUMN THAT EXISTS IN YOUR DATAFRAME\n",
    "\n",
    "# Standardize the 'FATAL' column (Crucial cleaning step for GSAF data)\n",
    "# Clean up the column by stripping whitespace and converting to uppercase.\n",
    "# We are looking for 'Y' (Yes) or 'F' (Fatal)\n",
    "df[FATAL_COLUMN] = df[FATAL_COLUMN].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Filter the DataFrame for Fatal Attacks\n",
    "# Select rows where the FATAL_COLUMN equals 'Y' OR 'F'\n",
    "fatal_attacks_df = df[(df[FATAL_COLUMN] == 'Y') | (df[FATAL_COLUMN] == 'F')].copy()\n",
    "\n",
    "# Display the count and the first few rows\n",
    "print(f\"Total Fatal Attacks Found: {len(fatal_attacks_df)}\")\n",
    "print(\"\\nFirst 5 Fatal Attacks Records:\")\n",
    "print(fatal_attacks_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05eb179",
   "metadata": {},
   "outputs": [],
   "source": [
    "FATAL_COLUMN = 'Unnamed: 22' # <--- Try this if 'Unnamed: 21' gives a count of 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef59301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the country with the most fatal attacks\n",
    "deadliest_countries = fatal_attacks_df['Country'].value_counts()\n",
    "\n",
    "print(\"\\nTop 5 Countries by Fatal Shark Attacks:\")\n",
    "print(deadliest_countries.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c555cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the next common placeholder column\n",
    "FATAL_COLUMN = 'Unnamed: 22' \n",
    "\n",
    "# Standardize the 'FATAL' column (Crucial cleaning step for GSAF data)\n",
    "# Clean up the column by stripping whitespace and converting to uppercase.\n",
    "df[FATAL_COLUMN] = df[FATAL_COLUMN].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Filter the DataFrame for Fatal Attacks\n",
    "# Select rows where the FATAL_COLUMN equals 'Y' OR 'F'\n",
    "fatal_attacks_df = df[(df[FATAL_COLUMN] == 'Y') | (df[FATAL_COLUMN] == 'F')].copy()\n",
    "\n",
    "# Display the count and the first few rows\n",
    "print(f\"Total Fatal Attacks Found: {len(fatal_attacks_df)}\")\n",
    "print(\"\\nFirst 5 Fatal Attacks Records:\")\n",
    "print(fatal_attacks_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a47883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FATAL_COLUMN = 'Injury'\n",
    "\n",
    "# Standardize the 'Injury' column\n",
    "df[FATAL_COLUMN] = df[FATAL_COLUMN].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Filter by checking if the injury string CONTAINS the word 'FATAL'\n",
    "# This is more robust for textual data.\n",
    "fatal_attacks_df = df[df[FATAL_COLUMN].str.contains('FATAL', na=False)].copy()\n",
    "\n",
    "# Display the count\n",
    "print(f\"Total Fatal Attacks Found (using Injury column): {len(fatal_attacks_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2798f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the country with the most fatal attacks\n",
    "# Group the fatal attacks DataFrame by 'Country' and count the incidents\n",
    "deadliest_countries = fatal_attacks_df['Country'].value_counts()\n",
    "\n",
    "# Print the top 5 deadliest countries\n",
    "print(\"\\nTop 5 Countries by Fatal Shark Attacks:\")\n",
    "print(deadliest_countries.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a46f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming 'deadliest_countries' contains the value_counts() you found:\n",
    "# AUSTRALIA: 310, USA: 202, etc.\n",
    "\n",
    "# Select only the top 5 countries\n",
    "top_5_fatal = deadliest_countries.head(5)\n",
    "\n",
    "# Initialize the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_5_fatal.index, y=top_5_fatal.values, palette=\"Reds_d\")\n",
    "# SABINA COMMENT: The shading should be the other way round, where darker colors = more attacks\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Top 5 Countries by Fatal Shark Attacks', fontsize=16, weight='bold')\n",
    "plt.xlabel('Country', fontsize=12)\n",
    "plt.ylabel('Number of Fatal Attacks', fontsize=12)\n",
    "\n",
    "# Add the count values on top of the bars\n",
    "for i, count in enumerate(top_5_fatal.values):\n",
    "    plt.text(i, count + 5, str(count), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Rotate x-axis labels for better visibility\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
